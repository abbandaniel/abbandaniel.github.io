---
layout: post
title: concrete dataset
author: Daniel Abban
published: true
status: publish
draft: false
tags: R
---
 
This is a model built for predicting **concrete compressive strength** - an important material in civil engineering.
 
 
The concrete compressive strength is a highly nonlinear function of **age** and 
**ingredients**. 
 
***
 
These ingredients includes: 
 
* cement 
* blast 
* furnace 
* slag
* fly ash 
* water 
* superplasticizer
* coarse aggregate 
* fine aggregate.
 
NB: The actual concrete compressive strength (MPa) for a *given mixture* under a 
specific age (days) was determined from laboratory. The data can be downloaded here: [UCI](https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/)
 
***
 
Load the requied packages for building the model:

{% highlight r %}
library(tidyverse)
library(readxl)
library(caret)
library(GGally)
library(rattle)
library(modelr)
library(partykit)
library(e1071)
{% endhighlight %}
 
***
 
Get the data into R:

{% highlight r %}
concrete <- read_excel("Concrete_Data.xls")
{% endhighlight %}



{% highlight text %}
## Error: 'Concrete_Data.xls' does not exist in current working directory ('C:/Users/daniel/Documents/GitHub/abbandaniel.github.io').
{% endhighlight %}
 
***
 
Rename the variables:

{% highlight r %}
names(concrete) <- c("cement", "blast_furnace",
                 "fly_ash", "water",
                 "superplasticizer",
                 "coarse_aggregate",
                 "fine_aggregate",
                 "age",
                 "concrete_strength")
{% endhighlight %}
 
***
 
Compute the skewness across the columns - If the predictor distribution is roughly symmetric, the skewness values will be close to zero
 

{% highlight r %}
skewValues <- map_dbl(concrete, skewness)
skewValues
{% endhighlight %}



{% highlight text %}
##            cement     blast_furnace           fly_ash             water 
##        0.50803436        0.79840662        0.53588075        0.07410764 
##  superplasticizer  coarse_aggregate    fine_aggregate               age 
##        0.90546945       -0.04008937       -0.25224294        3.25966169 
## concrete_strength 
##        0.41570873
{% endhighlight %}
 
The skewed variables have some values containing zero which makes it impossible to apply the BoxCox transformation
 
***
 
#### Get some exploratory graphs:
 

{% highlight r %}
ggplot(concrete, aes(concrete_strength)) +
        geom_histogram()
{% endhighlight %}

![plot of chunk unnamed-chunk-5](/figures/unnamed-chunk-5-1.png)

{% highlight r %}
ggplot(concrete, aes(x = concrete_strength, y = cement)) +
        geom_point()
{% endhighlight %}

![plot of chunk unnamed-chunk-5](/figures/unnamed-chunk-5-2.png)

{% highlight r %}
ggplot(concrete, aes(x = concrete_strength, y = coarse_aggregate)) +
        geom_point()
{% endhighlight %}

![plot of chunk unnamed-chunk-5](/figures/unnamed-chunk-5-3.png)
 
***
 
Seperate out the outcome from the predictors:

{% highlight r %}
concrete_strength <- concrete$concrete_strength
concrete <- select(concrete, -concrete_strength)
{% endhighlight %}
 
***
 
Partition the data into a training and a test set 

{% highlight r %}
idx <- createDataPartition(concrete_strength, p = 2/3, list = FALSE)
 
train_set <- concrete[idx, ]
train_outcome <- concrete_strength[idx]
 
test_set <- concrete[-idx, ]
test_outcome <- concrete_strength[-idx]
{% endhighlight %}
 
***
 
Fit a single linear model and conduct 10-fold cross-validation to estimate the error:

{% highlight r %}
ctrl <- trainControl(method = "cv")
 
set.seed(3)
lm_tune <- train(x = train_set, y = train_outcome,
                 method = "lm",
                 trControl = ctrl)
 
lm_tune
{% endhighlight %}



{% highlight text %}
## Linear Regression 
## 
## 688 samples
##   8 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 620, 618, 620, 619, 619, 620, ... 
## Resampling results:
## 
##   RMSE      Rsquared
##   10.47803  0.604791
## 
## 
{% endhighlight %}
 
***
 
predict the test set data and save the testset result in a data frame:

{% highlight r %}
test_result <- data.frame(observations = test_outcome, lm_predictions = predict(lm_tune, test_set))
{% endhighlight %}
 
***
 
Tune the regression tree using "rpart"

{% highlight r %}
set.seed(3)
tree_tune <- train(x = train_set, y = train_outcome,
                   method = "rpart",
                   trControl = ctrl,
                   tuneLength = 5)
plot(tree_tune)
{% endhighlight %}

![plot of chunk unnamed-chunk-10](/figures/unnamed-chunk-10-1.png)

{% highlight r %}
test_result$tree_predictions <- predict(tree_tune, test_set)
{% endhighlight %}
 
***
 
Plot the regression tree:

{% highlight r %}
fancyRpartPlot(tree_tune$finalModel)
{% endhighlight %}

![plot of chunk unnamed-chunk-11](/figures/unnamed-chunk-11-1.png)
 
***
 
Fit the multi-addaptive regression model:

{% highlight r %}
set.seed(3)
mars_tune <- train(x = train_set, y = train_outcome,
                   method = "earth",
                   trControl = ctrl,
                   tuneLength = 20)
 
plot(mars_tune)
{% endhighlight %}

![plot of chunk unnamed-chunk-12](/figures/unnamed-chunk-12-1.png)

{% highlight r %}
test_result$mars_predictions <- predict(mars_tune, test_set)
{% endhighlight %}
 
***
 
Compute and plot the variable importance

{% highlight r %}
marsImp <- varImp(mars_tune)
plot(marsImp)
{% endhighlight %}

![plot of chunk unnamed-chunk-13](/figures/unnamed-chunk-13-1.png)
 
***
 
Support vector machines:

{% highlight r %}
set.seed(3)
svm_tune <- train(x = train_set, y = train_outcome,
                   method = "svmRadial",
                   trControl = ctrl,
                  preProcess = c("center", "scale"),
                  tuneLength = 10)
 
 
 
test_result$svm_predictions <- predict(svm_tune, test_set)
{% endhighlight %}
 
***
 

{% highlight r %}
head(test_result)
{% endhighlight %}



{% highlight text %}
##   observations lm_predictions tree_predictions        y svm_predictions
## 1     40.26954       54.70608         40.01737 47.25089        41.66093
## 2     41.05278       64.49469         40.01737 47.35210        39.64184
## 3     36.44777       30.58470         56.73856 39.42591        37.18431
## 4     45.85429       19.69290         40.01737 29.84761        43.99643
## 5     28.02168       22.78867         40.01737 28.83276        28.71022
## 6     56.14196       55.71888         40.01737 36.86978        48.18204
{% endhighlight %}
 
***
 
#### Get test set performance values for each of the models

{% highlight r %}
postResample(pred = test_result$lm_predictions, obs = test_result$observations)
{% endhighlight %}



{% highlight text %}
##       RMSE   Rsquared 
## 10.5888743  0.6086991
{% endhighlight %}
 
***
 

{% highlight r %}
postResample(pred = test_result$tree_predictions, obs = test_result$observations)
{% endhighlight %}



{% highlight text %}
##       RMSE   Rsquared 
## 12.3099456  0.4739477
{% endhighlight %}
 
***
 

{% highlight r %}
postResample(pred = test_result$mars_predictions, obs = test_result$observations)
{% endhighlight %}



{% highlight text %}
##      RMSE  Rsquared 
## 7.0164577 0.8286503
{% endhighlight %}
 
***
 

{% highlight r %}
postResample(pred = test_result$svm_predictions, obs = test_result$observations)
{% endhighlight %}



{% highlight text %}
##      RMSE  Rsquared 
## 5.9437153 0.8821154
{% endhighlight %}
 
 
