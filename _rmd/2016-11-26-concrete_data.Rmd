---
title: "Concrete-compresive-strength"
author: "Daniel Abban"
date: "10 October 2016"
output: html_document
---

This is a model built for predicting **concrete compressive strength** - an important material in civil engineering.

***

The concrete compressive strength is a highly nonlinear function of **age** and 
**ingredients**. 

***

These ingredients includes: 

* cement 
* blast 
* furnace 
* slag
* fly ash 
* water 
* superplasticizer
* coarse aggregate 
* fine aggregate.

NB: The actual concrete compressive strength (MPa) for a *given mixture* under a 
specific age (days) was determined from laboratory. The data can be downloaded here: [UCI](https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/)

***

Load the requied packages for building the model:
```{r message=FALSE}
library(tidyverse)
library(readxl)
library(caret)
library(GGally)
library(rattle)
library(modelr)
library(partykit)
library(e1071)
```

***

Get the data into R:
```{r}
concrete <- read_excel("Concrete_Data.xls")
```

***

Rename the variables:
```{r }
names(concrete) <- c("cement", "blast_furnace",
                 "fly_ash", "water",
                 "superplasticizer",
                 "coarse_aggregate",
                 "fine_aggregate",
                 "age",
                 "concrete_strength")
```

***

Compute the skewness across the columns - If the predictor distribution is roughly symmetric, the skewness values will be close to zero

```{r}
skewValues <- map_dbl(concrete, skewness)
skewValues
```

The skewed variables have some values containing zero which makes it impossible to apply the BoxCox transformation

***

#### Get some exploratory graphs:

```{r message=FALSE}

ggplot(concrete, aes(concrete_strength)) +
        geom_histogram()

ggplot(concrete, aes(x = concrete_strength, y = cement)) +
        geom_point()

ggplot(concrete, aes(x = concrete_strength, y = coarse_aggregate)) +
        geom_point()
```

***

Seperate out the outcome from the predictors:
```{r}
concrete_strength <- concrete$concrete_strength
concrete <- select(concrete, -concrete_strength)
```

***

Partition the data into a training and a test set 
```{r}
idx <- createDataPartition(concrete_strength, p = 2/3, list = FALSE)

train_set <- concrete[idx, ]
train_outcome <- concrete_strength[idx]

test_set <- concrete[-idx, ]
test_outcome <- concrete_strength[-idx]
```

***

Fit a single linear model and conduct 10-fold cross-validation to estimate the error:
```{r}
ctrl <- trainControl(method = "cv")

set.seed(3)
lm_tune <- train(x = train_set, y = train_outcome,
                 method = "lm",
                 trControl = ctrl)

lm_tune
```

***

predict the test set data and save the testset result in a data frame:
```{r}
test_result <- data.frame(observations = test_outcome, lm_predictions = predict(lm_tune, test_set))
```

***

Tune the regression tree using "rpart"
```{r message=FALSE, warning=FALSE}

set.seed(3)
tree_tune <- train(x = train_set, y = train_outcome,
                   method = "rpart",
                   trControl = ctrl,
                   tuneLength = 5)
plot(tree_tune)

test_result$tree_predictions <- predict(tree_tune, test_set)
```

***

Plot the regression tree:
```{r}
fancyRpartPlot(tree_tune$finalModel)
```

***

Fit the multi-addaptive regression model:
```{r message=FALSE}
set.seed(3)
mars_tune <- train(x = train_set, y = train_outcome,
                   method = "earth",
                   trControl = ctrl,
                   tuneLength = 20)

plot(mars_tune)

test_result$mars_predictions <- predict(mars_tune, test_set)
```

***

Compute and plot the variable importance
```{r}
marsImp <- varImp(mars_tune)
plot(marsImp)
```

***

Support vector machines:
```{r message=FALSE}
set.seed(3)
svm_tune <- train(x = train_set, y = train_outcome,
                   method = "svmRadial",
                   trControl = ctrl,
                  preProcess = c("center", "scale"),
                  tuneLength = 10)



test_result$svm_predictions <- predict(svm_tune, test_set)
```

***

```{r }
head(test_result)
```

***

#### Get test set performance values for each of the models
```{r}
postResample(pred = test_result$lm_predictions, obs = test_result$observations)
```

***

```{r}
postResample(pred = test_result$tree_predictions, obs = test_result$observations)
```

***

```{r}
postResample(pred = test_result$mars_predictions, obs = test_result$observations)
```

***

```{r}
postResample(pred = test_result$svm_predictions, obs = test_result$observations)
```


